{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cbbb9-b14e-4ec5-8840-6da56bbcf06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef44f3d-f209-4f75-adf4-c4a43a802871",
   "metadata": {},
   "source": [
    "### Embeddings/Feature Extraction\n",
    "\n",
    "Feature extraction means obtaining the embedding vectors for a given text from a pre-trained model.  Once you have the embeddings, which are numerical representations of text, lots of possibilities open up.  You can compare the similarity between documents, you can use the embeddings to match questions to answers, perform clustering based on any algorithm, use the embeddings as features to create clusters of similar documents, and so on.\n",
    "\n",
    "**Difference between word embeddings and document embeddings**  \n",
    "So far, we have been talking of word embeddings, which means we have a large embedding vector for every single word in our text data.  What do we mean when we say sentence or document embedding?  A sentence's embedding is derived from the embeddings for all the words in the sentence.  The embedding vectors are generally averaged ('mean-pooled'), though other techniques such as 'max-pooling' are also available.  It is surprising that we spend so much effort computing separate embeddings for words considering context and word order, and then just mash everything up using an average to get a single vector for the entire sentence, or even the document.  It is equally surprising that this approach works remarkably effectively for a large number of tasks.\n",
    "\n",
    "Fortunately for us, the sentence transformers library knows how to computer mean-pooled or other representations of entire documents based upon the pre-trained model used.  Effectively, we reduce the entire document to a single vector that may have 768 or such number of dimensions.\n",
    "\n",
    "Let us look at this in action.\n",
    "\n",
    "First, we get embeddings for our corpus using a specific model.  We use the 'all-MiniLM-L6-v2' for symmetric queries, and any of the MSMARCO models for asymmetric queries.  The difference between symmetric an asymmetric queries is that the query and the sentences are roughly the same length in symmetric queries.  In asymmetric queries, the query is much smaller than the sentences.\n",
    "\n",
    "This is based upon the documentation on sentence-bert's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dac97b-c3c4-42e4-a585-73ad45f9a9db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Toy example with just three sentences to see what embeddings look like\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2') #for symmetric queries\n",
    "model = SentenceTransformer('msmarco-distilroberta-base-v2') #for asymmetric queries\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e4ef3-3bad-4ffd-a656-00328978abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87953b-7b4e-440a-94dd-b40f9ec2041b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use our data\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') #for symmetric queries\n",
    "# model = SentenceTransformer('msmarco-distilroberta-base-v2') #for asymmetric queries\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = list(corpus.text)\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14181f58-a4d0-4cbc-b099-6f1266fad175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, the variable embeddings contains all our embeddings, one row for each document\n",
    "# So we expect there to be 100 rows, and as many columns as the model we chose vectorizes text\n",
    "# into.\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bc283-d824-4339-84c3-ab1eaab449eb",
   "metadata": {},
   "source": [
    "### Cosine similarity between sentences \n",
    "\n",
    "We can compute the cosine similarity between documents, and that gives us a measure of how similar sentences or documents are.\n",
    "\n",
    "The below code uses brute force, and finds the most similar sentences.  Very compute intensive, will not run if number of sentences is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef8d25-04b5-462a-84bb-016b539d45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "distances = util.cos_sim(embeddings, embeddings)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07910c6-fac1-4653-9017-6ea076ea70fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dist = pd.DataFrame(distances, columns = corpus.index, index = corpus.index)\n",
    "df_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7bc19-a920-4ca7-b311-4dfe8d5a7ae3",
   "metadata": {},
   "source": [
    "At this point, we can use `stack` to rearrange the data to identify similar articles, but `stack` fails if you have a lot of documents.  Let us see how `stack` does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81514035-a708-4441-8bdf-75bdd5b22a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stack\n",
    "df_dist = df_dist.stack().reset_index()\n",
    "df_dist.columns = ['article', 'similar_article', 'similarity']\n",
    "df_dist = df_dist.sort_values(by = ['article', 'similarity'], ascending = [True, False])\n",
    "df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51b28d-876a-42e6-be81-cd010ae483f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us reset our df_dist dataframe\n",
    "df_dist = pd.DataFrame(distances, columns = corpus.index, index = corpus.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf8485-6ffe-4088-b031-abc1cb5edc6b",
   "metadata": {},
   "source": [
    "Stack will fail if the number of documents is large.  In that case, we decide the number of top similar documents (say 20), that we need identified for each document.\n",
    "\n",
    "We do this using the below loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245fcb9-652e-4732-890a-02481694c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a loop\n",
    "\n",
    "\n",
    "\n",
    "top_n = 21\n",
    "temp = []\n",
    "for col in tqdm(range(len(df_dist))):\n",
    "    t = pd.DataFrame(df_dist.iloc[:, col].sort_values(ascending = False)[:top_n]).stack().reset_index()\n",
    "    t.columns = ['similar_article', 'article', 'similarity']\n",
    "    t = t[['article', 'similar_article', 'similarity']]\n",
    "    temp.append(t)\n",
    "\n",
    "pd.concat(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
