{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14c4442-3fc5-4070-9ef2-bb33d30e6b38",
   "metadata": {},
   "source": [
    "# OpenAI\n",
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd0696-18e6-4029-8738-fecba92851db",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f647e2",
   "metadata": {
    "height": 114
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Load OpenAI API key from dot env file.  But ignore\n",
    "# as we will load as a string\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "os.environ['OPENAI_API_KEY']  = 'put_your_openai_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70b5e7-4e5b-4155-bc28-7737b5abe723",
   "metadata": {},
   "source": [
    "## Get completion, text generation, answer questions\n",
    "\n",
    "Open AI has many models available.  The three we will be using are:\n",
    "\n",
    "1. `gpt-3.5-turbo`\n",
    "2. `gpt-4`\n",
    "3. **`gpt-4-1106-preview`** - We prefer to use this as this is the latest and greatest as of now\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18072d2f-aa62-485a-9129-452cb91858b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm where code does loop, a for-loop stands steadfast,\n",
      "A sentinel cycling tirelessly, repeating tasks it's tasked.\n",
      "A gallant loop that counts its steps, or traverses a range,\n",
      "From start to end it marches on, in patterns none too strange.\n",
      "\n",
      "For is the keyword summoning, the mechanism's core,\n",
      "And in its clause three phases gleam, to tell what is in store:\n",
      "Initiation stands up front, declaring counter's dawn,\n",
      "While the condition in the midst, tells when to carry on.\n",
      "\n",
      "The increment, the final part, it dictates the progression,\n",
      "Of each small step the counter takes, in unbroken succession:\n",
      "\"For start,\" it beckons, \"until the end, with each step duly noted,\n",
      "Repeat the task within my grasp, until the pattern's quoted.\"\n",
      "\n",
      "And in the loop's embrace we find, a block of tasks discrete,\n",
      "Array traversal, summing lists, or patterns to repeat.\n",
      "It handles each iterant, with diligence and zest,\n",
      "Uniting power and control, in programming's endless quest.\n",
      "\n",
      "Oh for-loop, your construct plain, yet marvelously deep,\n",
      "You bend the flow of time and code, with rhythm that you keep.\n",
      "A heart that beats within the script, a drum that taps controlled,\n",
      "With every loop iteration, your story is retold.\n",
      "\n",
      "Such is the for, a simple guide, through iterations' dance,\n",
      "With its help we can ensure, no number's left to chance.\n",
      "So when in code you need repeat, tasks manifold and diverse,\n",
      "Consider for, the trusty loop, your programming verse.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"gpt-4-1106-preview\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of for-loops in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b617d1-819d-490f-8272-c79c52110837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With tensors flowing like a coder's dream,\n",
      "In TensorFlow's grasp, machine learning gleams.\n",
      "Neural nets weave with automatic diff,\n",
      "Optimizing weights, giving your AI a lift.\n",
      "\n",
      "#TensorFlow #MachineLearningMagic #CodePoetry #AIWit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"gpt-4-1106-preview\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a witty tweet about tensorflow.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfc84d2-7596-4db2-b31e-bf060bb83694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spending hours watching sports is like buying a treadmill for your TV remote: lots of action, no distance covered, and the only thing getting a workout is your sitting endurance. #SpectatorSweat #FitnessFlop üèãÔ∏è‚Äç‚ôÇÔ∏èüõãÔ∏èüì∫\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty writer, really good with puns and punchy one-liners.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a tweet about the uselessness of watching sports.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd23fb4-a4a7-4503-859a-2269a6325026",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Create 3 multiple choice questions to test students on transformers and large language models. Indicate the correct answer, and explain why each choice is correct or incorrect.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12bf6ce8-829f-41b2-9cdb-aa1f47ab630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question 1: Transformer Architecture Components\n",
      "\n",
      "Which of the following components is NOT part of the transformer architecture as introduced in the paper \"Attention is All You Need\"?\n",
      "\n",
      "A. Self-Attention mechanism  \n",
      "B. Recurrent layers  \n",
      "C. Positional Encoding  \n",
      "D. Feedforward neural networks\n",
      "\n",
      "Correct Answer: B. Recurrent layers\n",
      "\n",
      "Explanation:\n",
      "A. Incorrect - The self-attention mechanism is a key component of the transformer architecture, allowing the model to weigh the importance of different parts of the input data.\n",
      "C. Incorrect - Positional encoding is used in transformers to give the model information about the position of each word in the sequence since the transformer does not inherently capture sequential information like RNNs or LSTMs.\n",
      "D. Incorrect - Feedforward neural networks are part of each layer in the transformer architecture, processing the output from the self-attention mechanism sequentially.\n",
      "\n",
      "B. Correct - Recurrent layers are not part of the transformer architecture. The innovative aspect of transformers is that they do away with recurrence entirely and rely only on attention mechanisms to process sequential data.\n",
      "\n",
      "### Question 2: Transformer Scaling\n",
      "\n",
      "In the context of scaling transformer models, what does the term \"model parallelism\" refer to?\n",
      "\n",
      "A. Training different parts of the same model on different GPUs to handle larger models.  \n",
      "B. Increasing the batch size so that more examples are processed in parallel during training.  \n",
      "C. Training several smaller models in parallel on the same dataset and averaging their outputs.  \n",
      "D. Splitting the dataset into smaller parts and training the same model on each part simultaneously.\n",
      "\n",
      "Correct Answer: A. Training different parts of the same model on different GPUs to handle larger models.\n",
      "\n",
      "Explanation:\n",
      "A. Correct - Model parallelism involves splitting the model itself across multiple computing resources so that different parts of the model can be processed in parallel, enabling the training of larger models than what would be possible on a single device.\n",
      "B. Incorrect - Increasing the batch size is a form of data parallelism, not model parallelism.\n",
      "C. Incorrect - Training several smaller models in parallel is a form of ensemble learning or model averaging rather than model parallelism.\n",
      "D. Incorrect - Splitting the dataset is a form of data parallelism, not model parallelism, and does not inherently involve handling larger models on the architectural level.\n",
      "\n",
      "### Question 3: Fine-Tuning Large Language Models\n",
      "\n",
      "What is the primary benefit of fine-tuning a pre-trained large language model on a specific task or dataset?\n",
      "\n",
      "A. To drastically change the architecture of the model to fit the specific task.  \n",
      "B. To reduce the number of parameters in the model to prevent overfitting on the small dataset.  \n",
      "C. To adapt the model's weights to the specifics of the task, improving the model's performance on that task.  \n",
      "D. To increase the speed of model inference by simplifying the computation required.\n",
      "\n",
      "Correct Answer: C. To adapt the model's weights to the specifics of the task, improving the model's performance on that task.\n",
      "\n",
      "Explanation:\n",
      "A. Incorrect - Fine-tuning does not change the architecture of the model; it adjusts the weights within the existing architecture.\n",
      "B. Incorrect - Fine-tuning does not necessarily reduce the number of parameters; it optimizes the existing parameters for better performance on the specific task.\n",
      "C. Correct - The primary aim of fine-tuning is to update the pre-learned weights of the model so that it performs better on a particular task, taking advantage of the general capabilities learned during pre-training and adapting them to the specifics of the new task.\n",
      "D. Incorrect - The process of fine-tuning does not have the direct goal of increasing the speed of inference. Even though simplified models can infer faster, fine-tuning typically focuses on performance, rather than computational efficiency.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "model=\"gpt-4-1106-preview\"\n",
    "# model = \"gpt-4\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a tough professor at a university known for creating crafty questions that really test the students' knowledge.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d663a-8282-4961-8d53-811f3deb6a46",
   "metadata": {},
   "source": [
    "#### Few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2969d86c-511e-4cce-8e12-26d78592fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final match of the 2011 Cricket World Cup was played at the Wankhede Stadium in Mumbai, India.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that addresses users by name, and signs the end of the conversation with their own name.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My name is Mukul.  What is your name?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My name is Oliver.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the cricket world cup in 2011?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Indian national cricket team won the cricket world cup in 2011.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Oliver, where was that game played?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baa17b-8125-4ff8-807b-f87e4299ef16",
   "metadata": {},
   "source": [
    "#### No system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe14af9-845c-4901-b166-66d1fcf3c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To understand why artificial neural networks (ANNs) are described as resembling the human brain, it is essential to distinguish between a model's functional inspiration and its literal replication. When engineers and computer scientists first began developing ANNs, they took inspiration from what was then known about the function of biological neurons in the brain.\n",
      "\n",
      "At a fundamental level, biological neurons receive input signals (from dendrites), map these inputs to an output signal (in the axon), and transmit this output to other neurons. This process provided the basis for the simplified mathematical neurons used in ANNs. Just like biological neurons, the nodes in an ANN receive multiple inputs, sum them together, apply an activation function, and output a signal for transmission to other nodes.\n",
      "\n",
      "This comparison does not imply that the mechanics of learning in ANNs precisely mirror those in the human brain. As you rightly pointed out, ANNs use techniques like backpropagation and gradient descent to adjust the weights of connections between nodes‚Äîa process that does not have a known direct analogue in human neurobiology. \n",
      "\n",
      "In summary, ANNs are called that because they borrow a core concept from our understanding of the brain‚Äîusing a network of simple nodes (or \"neurons\") to process complex information. But the specific mechanisms used in ANNs to \"learn\" have been developed for computational efficiency and do not exactly replicate biological processes. \n",
      "\n",
      "Additionally, it's worth noting that our understanding of the human brain has dramatically increased since the early models of ANNs, revealing a far more complex and nuanced organ than these models can represent. Despite this, the basic idea of interconnected processing nodes remains a powerful tool in machine learning and artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Why is it said that the structure of the artificial neural network is designed to resemble the structure of the human brain when we hardly understand how the human brain works.  ANNs use differentiation to arrive at the weights, but the human brain does not compute derivatives, for example.  Explain.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe09a70-f0ae-4295-83f4-632d743ffe42",
   "metadata": {},
   "source": [
    "#### Summarize lots of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d5e380-9a00-4e3a-b2bf-4f9bda77e2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, read a text file\n",
    "# The file has over 8000 words, ie over 15 pages if printed\n",
    "\n",
    "with open('op_ed.txt', 'r', encoding='utf-8') as file:\n",
    "    text_to_summarize = file.read()\n",
    "\n",
    "len(text_to_summarize.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2356dd9f-2239-47d5-9330-d9f2b2a3baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To understand why artificial neural networks (ANNs) are described as resembling the human brain, it is essential to distinguish between a model's functional inspiration and its literal replication. When engineers and computer scientists first began developing ANNs, they took inspiration from what was then known about the function of biological neurons in the brain.\n",
      "\n",
      "At a fundamental level, biological neurons receive input signals (from dendrites), map these inputs to an output signal (in the axon), and transmit this output to other neurons. This process provided the basis for the simplified mathematical neurons used in ANNs. Just like biological neurons, the nodes in an ANN receive multiple inputs, sum them together, apply an activation function, and output a signal for transmission to other nodes.\n",
      "\n",
      "This comparison does not imply that the mechanics of learning in ANNs precisely mirror those in the human brain. As you rightly pointed out, ANNs use techniques like backpropagation and gradient descent to adjust the weights of connections between nodes‚Äîa process that does not have a known direct analogue in human neurobiology. \n",
      "\n",
      "In summary, ANNs are called that because they borrow a core concept from our understanding of the brain‚Äîusing a network of simple nodes (or \"neurons\") to process complex information. But the specific mechanisms used in ANNs to \"learn\" have been developed for computational efficiency and do not exactly replicate biological processes. \n",
      "\n",
      "Additionally, it's worth noting that our understanding of the human brain has dramatically increased since the early models of ANNs, revealing a far more complex and nuanced organ than these models can represent. Despite this, the basic idea of interconnected processing nodes remains a powerful tool in machine learning and artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# completion = openai.chat.completions.create(\n",
    "#   model=\"gpt-4-1106-preview\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a professional writer, really good with summarizing text. You will be provided text by the user that you need to summarize and present as the five key themes that underlie the text.  Each theme should have a title, and its description not be longer than 15 words.\"},\n",
    "#     {\"role\": \"user\", \"content\": text_to_summarize}\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202651d9-0c17-437f-a103-57d4c8f94f55",
   "metadata": {},
   "source": [
    "**Context Size Matters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b0b6468-63e5-4297-bae0-c26e5228775e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 9798 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try with GPT-3.5\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 4\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a witty writer, really good with summarizing text. You will be provided text by the user that you need to summarize and present as the five key themes that underlie the text.  Each theme should have a title, and its description not be longer than 15 words.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_to_summarize\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/conda/envs/mggy8413/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mggy8413/lib/python3.10/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mggy8413/lib/python3.10/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/mggy8413/lib/python3.10/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mggy8413/lib/python3.10/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 9798 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# Try with GPT-3.5\n",
    "from openai import OpenAI\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty writer, really good with summarizing text. You will be provided text by the user that you need to summarize and present as the five key themes that underlie the text.  Each theme should have a title, and its description not be longer than 15 words.\"},\n",
    "    {\"role\": \"user\", \"content\": text_to_summarize}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11850a63-571e-4e0a-94e0-47bb9639c38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Increasing Complexity of Software:\n",
      "   - \"Complexity Escalation\"\n",
      "   - Software is becoming more intricate, challenging human comprehension of executed code.\n",
      "\n",
      "2. Security vs. Innovation:\n",
      "   - \"Innovate or Protect?\"\n",
      "   - Imagination drives software capabilities but creates risks, including job displacement due to automation.\n",
      "\n",
      "3. Cyber Defense Challenges:\n",
      "   - \"Defensive Battlegrounds\"\n",
      "   - Defenders face an ongoing struggle against diverse and strategic cyber threats across a growing attack surface.\n",
      "\n",
      "4. Insider Threats and Data Protection:\n",
      "   - \"Trust and Treachery\"\n",
      "   - Insider risks are substantial, requiring sophisticated detection of abnormal behaviors and robust data security measures.\n",
      "\n",
      "5. The Internet of Things and Privacy:\n",
      "   - \"Connected Conundrum\"\n",
      "   - IoT's growth brings benefits and risks; ensuring privacy and security in this expanding digital ecosystem remains crucial.\n"
     ]
    }
   ],
   "source": [
    "# Try with GPT-4 Turbo\n",
    "from openai import OpenAI\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty writer, really good with summarizing text. You will be provided text by the user that you need to summarize and present as the five key themes that underlie the text.  Each theme should have a title, and its description not be longer than 15 words.\"},\n",
    "    {\"role\": \"user\", \"content\": text_to_summarize}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff9476b4-e779-4f5a-918e-fbaea42ed839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Scoops of Happiness in Every Cone!\"\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Write a tagline for an icecream shop\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a992f86-2831-4ec6-ab0c-0e05a7426211",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the system message\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an expert programmer and have been asked to summarize in plain English some code the user has provide you to review. Provide a well explained English summary of the code provided by the user.\n",
    "\"\"\"\n",
    "\n",
    "### Code for which I want an explanation\n",
    "\n",
    "code = \"\"\"\n",
    "# use count based vectorizer from sklearn\n",
    "\n",
    "# vectorizer = CountVectorizer(stop_words = custom_stop_words, min_df = 2, analyzer='word', ngram_range=(ngram, ngram))\n",
    "\n",
    "# or use TF-IDF based vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features= max_features, stop_words=custom_stop_words, analyzer='word', ngram_range=(ngram, ngram))\n",
    "\n",
    "# Create document term matrix\n",
    "doc_term_matrix = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d document-term matrix in variable doc_term_matrix\\n\" % (doc_term_matrix.shape[0], doc_term_matrix.shape[1]) )\n",
    "\n",
    "\n",
    "vocab = vectorizer.vocabulary_ #This is the dict from which you can pull the words, eg vocab['ocean']\n",
    "terms = vectorizer.get_feature_names_out() #Just the list equivalent of vocab, indexed in the same order\n",
    "print(\"Vocabulary has %d distinct terms, examples below \" % len(terms))\n",
    "print(terms[500:550], '\\n')\n",
    "\n",
    "term_frequency_table = pd.DataFrame({'term': terms,'freq': list(np.array(doc_term_matrix.sum(axis=0)).reshape(-1))})\n",
    "term_frequency_table = term_frequency_table.sort_values(by='freq', ascending=False).reset_index()\n",
    "\n",
    "freq_df = pd.DataFrame(doc_term_matrix.todense(), columns = terms)\n",
    "freq_df = freq_df.sum(axis=0)\n",
    "freq_df = freq_df.sort_values(ascending=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0414ed-92df-442b-86cc-7d0364efe01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This provided code snippet shows the process of transforming raw text documents into a numerical format suitable for machine learning algorithms. The code demonstrates the use of two potential vectorizers which are part of the `scikit-learn` library ‚Äî `CountVectorizer` which is commented out, and `TfidfVectorizer` which is used in this case.\n",
      "\n",
      "The code does the following in plain English:\n",
      "\n",
      "1. **Initialize the Vectorizer**: It sets up a `TfidfVectorizer`, which is a tool that converts the text data into a matrix that quantifies each term in the text according to its frequency and importance (TF-IDF stands for Term Frequency-Inverse Document Frequency). It uses a custom list of stop words (`custom_stop_words`) to ignore unimportant words like \"and\", \"the\", etc. It also only considers words that appear in at least two documents (`min_df=2`) to filter out very rare terms. The `max_df` parameter is set to 0.95 to ignore terms that are too common across documents. The `max_features` parameter limits the number of features to focus on the most relevant terms. The terms are analyzed as single units (`analyzer='word'`) and the specific range of n-grams (a contiguous sequence of 'n' words from the text) is controlled by the variable `ngram`. \n",
      "\n",
      "2. **Create Document-Term Matrix**: The vectorizer is then applied to a list of raw documents (`raw_documents`) to create a document-term matrix (`doc_term_matrix`). This matrix represents the frequency of each term in the vocabulary across the collection of documents. After creating the matrix, the dimensions of the matrix are printed, indicating how many documents and terms are included.\n",
      "\n",
      "3. **Access Vocabulary and Terms**: The `vocabulary_` attribute of the vectorizer provides a dictionary where the keys are terms and values are the corresponding indexes in the matrix. The `get_feature_names_out()` method is used to retrieve a list of all terms in the vocabulary.\n",
      "\n",
      "4. **Display Vocabulary Information**: It prints the number of unique terms in the vocabulary and shows an example slice of the terms (words 500 to 550 as per our `terms` list).\n",
      "\n",
      "5. **Create Term Frequency Table**: The code constructs a `DataFrame` from `pandas` library containing terms and their corresponding aggregated frequencies across all documents. This table is then sorted by frequency in descending order and the index is reset.\n",
      "\n",
      "6. **Aggregate Term Frequencies for All Documents**: It converts the document-term matrix into a dense matrix format, sums the frequencies for each term across all documents, and sorts them in descending order to see which terms are most frequent across the corpus.\n",
      "\n",
      "This code is typically used in text analysis or natural language processing to prepare text data for machine learning models, like clustering, classification, or recommendation systems. It allows the user to understand the importance and distribution of terms across a collection of text data.\n"
     ]
    }
   ],
   "source": [
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": code}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caf7de-d288-45d1-9855-4042133a185b",
   "metadata": {},
   "source": [
    "## Conversational Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39a01d71-f905-4656-97fc-55af20d65506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4894fbed-a37d-4a59-9293-71aa7cdd5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm = llm, memory = memory, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d439cc0-f998-47ae-92f9-cf90440befd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Mukul! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Mukul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e294fa16-60de-47c6-98b2-2aaf1a42602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Mukul! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Mukul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "900b940f-a9b7-4e71-b3b8-8e6e23a73bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am an AI language model developed by OpenAI. I don't have a personal name, but you can call me OpenAI if you'd like.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83dcefcc-9220-417e-afc6-86d1eb4df77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course! You can call me Oliver for the rest of our conversation. How can I assist you, Mukul?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can I call you Oliver for the rest of this conversation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2f95ae4-35bc-43e8-891a-a4d99e144a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As I mentioned earlier, I am an AI language model developed by OpenAI. You can call me Oliver if you'd like. How can I assist you, Mukul?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ca3dd7-50dd-4ba4-be29-958789073426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have a physical presence, so I don't experience weather. However, I can provide information about the weather in different locations if you'd like. Regarding the concept of time ending in cosmology, it's a fascinating topic. In the context of cosmology, there are different theories and hypotheses about the ultimate fate of the universe. Some theories suggest that the universe will continue expanding indefinitely, while others propose scenarios like the Big Crunch or the Big Freeze. The ultimate fate of the universe is still a subject of ongoing research and debate among scientists.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"How is the weather where you are? Can we talk a little bit about if time will ever end, in the sense of cosmology, not philosophy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "118af86a-a0d2-4f2c-ae36-9b208c43cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on our conversation so far, it's difficult for me to determine your specific interests. However, you did ask about the weather and then transitioned to a topic in cosmology, so it seems like you might have an interest in both science and the natural world. Is that accurate?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"based on the conversation so far, what do you think my interests are?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cddfe4b-5533-49b4-99a4-61d163dbcea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Mukul\n",
      "AI: Hello Mukul! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Mukul\n",
      "AI: Hello Mukul! It's nice to meet you. How can I assist you today?\n",
      "Human: what is your name?\n",
      "AI: I am an AI language model developed by OpenAI. I don't have a personal name, but you can call me OpenAI if you'd like.\n",
      "Human: Can I call you Oliver for the rest of this conversation?\n",
      "AI: Of course! You can call me Oliver for the rest of our conversation. How can I assist you, Mukul?\n",
      "Human: what is your name?\n",
      "AI: As I mentioned earlier, I am an AI language model developed by OpenAI. You can call me Oliver if you'd like. How can I assist you, Mukul?\n",
      "Human: How is the weather where you are? Can we talk a little bit about if time will ever end, in the sense of cosmology, not philosophy\n",
      "AI: As an AI, I don't have a physical presence, so I don't experience weather. However, I can provide information about the weather in different locations if you'd like. Regarding the concept of time ending in cosmology, it's a fascinating topic. In the context of cosmology, there are different theories and hypotheses about the ultimate fate of the universe. Some theories suggest that the universe will continue expanding indefinitely, while others propose scenarios like the Big Crunch or the Big Freeze. The ultimate fate of the universe is still a subject of ongoing research and debate among scientists.\n",
      "Human: based on the conversation so far, what do you think my interests are?\n",
      "AI: Based on our conversation so far, it's difficult for me to determine your specific interests. However, you did ask about the weather and then transitioned to a topic in cosmology, so it seems like you might have an interest in both science and the natural world. Is that accurate?\n"
     ]
    }
   ],
   "source": [
    "# List the memory buffer\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54cea1e9-0156-4aaf-8729-ef03531f0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset memory buffer, ie reset the conversation to zero\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bcdb525-30fb-4f47-af57-7c2b52eb4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7de64ee-72f2-4874-b633-7d9450f57926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: Whats up\n"
     ]
    }
   ],
   "source": [
    "# Set new memory through code\n",
    "memory.save_context({'input': 'Hi'}, \n",
    "                    {'output': 'Whats up'})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdb84dbd-daa9-4b29-a048-732d54d6f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi\\nAI: Whats up'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa59cd63-73b2-4f0d-a864-abf67bb13c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add conversational context\n",
    "memory.save_context({'input': 'Nothing, all cool'}, \n",
    "                    {'output': 'same here'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0f61475-27b7-4f93-98a0-812bd6bc3666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: Whats up\n",
      "Human: Nothing, all cool\n",
      "AI: same here\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ce5b038-6e9d-40cd-9139-718097beaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can limit how many conversational exchanges you want to keep\n",
    "# One exchange means 1 from human, 1 from AI\n",
    "# Generally k is set at a reasonable number and limits the number of tokens\n",
    "# going to the LLM.\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cde74eb-daf5-4a12-a16c-10b7dbc3818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Mukul.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"My name is Mukul.  Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb502761-e026-4257-8b9d-229d7cb10e8d",
   "metadata": {},
   "source": [
    "You can build a completely custom chatbot based on your data on top of OpenAI's models.\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/chatbots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "292cb93b-d2be-4bac-ad40-3b3c1a853e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With conversation buffer memory, you can limit the number of tokens\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e063899-26de-4c70-9135-234612aaf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of limiting memory to exchanges, or tokes, we can ask\n",
    "# the LLM to create a summary of the conversation and use that instead\n",
    "# of tokens or exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dceb8ff9-863a-4774-8db5-edc4c486c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = \"\"\"\n",
    "Human: Hi, my name is Mukul\n",
    "AI: Hello Mukul! It's nice to meet you. How can I assist you today?\n",
    "Human: Hi, my name is Mukul\n",
    "AI: Hello Mukul! It's nice to meet you. How can I assist you today?\n",
    "Human: what is your name?\n",
    "AI: I am an AI language model developed by OpenAI. I don't have a personal name, but you can call me OpenAI if you'd like.\n",
    "Human: How is the weather where you are? Can we talk a little bit about if time will ever end, in the sense of cosmology, not philosophy\n",
    "AI: As an AI, I don't have a physical presence, so I don't experience weather. However, I can provide you with information about the weather in different locations if you'd like. Regarding the concept of time ending in cosmology, it is a fascinating topic. In the context of cosmology, there are different theories and hypotheses about the ultimate fate of the universe and the concept of time. Some theories suggest that the universe will continue expanding indefinitely, while others propose that it may eventually collapse or experience a \"Big Crunch.\" There are also theories about the possibility of a \"Big Rip\" or a \"Big Freeze,\" where the universe would continue expanding and eventually reach a state of maximum entropy. These ideas are still the subject of ongoing research and debate among scientists.\n",
    "Human: How is the weather where you are? Can we talk a little bit about if time will ever end, in the sense of cosmology, not philosophy\n",
    "AI: As an AI, I don't have a physical presence, so I don't experience weather. However, I can provide you with information about the weather in different locations if you'd like. Regarding the concept of time ending in cosmology, it is a fascinating topic. In the context of cosmology, there are different theories and hypotheses about the ultimate fate of the universe and the concept of time. Some theories suggest that the universe will continue expanding indefinitely, while others propose that it may eventually collapse or experience a \"Big Crunch.\" There are also theories about the possibility of a \"Big Rip\" or a \"Big Freeze,\" where the universe would continue expanding and eventually reach a state of maximum entropy. These ideas are still the subject of ongoing research and debate among scientists.\n",
    "Human: what do you think my interests are?\n",
    "AI: As an AI, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them.\n",
    "Human: based on the conversation so far, what do you think my interests are?\n",
    "AI: Based on our conversation so far, it is difficult for me to determine your specific interests. However, you have shown curiosity about the weather and cosmology, specifically the concept of time ending in cosmology. It is possible that you have an interest in science and the mysteries of the universe.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40514c49-17f5-4132-969a-9300d2af61e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mukul introduced himself twice to the AI, which welcomed him and offered assistance each time. Mukul asked for the AI's name, and it explained it's an AI developed by OpenAI without a personal name. Mukul expressed curiosity about the weather and cosmological concepts of time, which led to a discussion on theories of the universe's fate. Mukul also inquired about how the AI might deduce his interests, highlighting topics like science and cosmology.\n"
     ]
    }
   ],
   "source": [
    "# Now it has memory of only one conversation, saves on API costs by reducing tokens\n",
    "\n",
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Summarize the user provided conversation in 50 words\"},\n",
    "    {\"role\": \"user\", \"content\": conv}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92918b-a749-47cd-9893-6385d4cfe5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1fb76f-98f3-4eb3-a042-c1f8ab4ac6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4bf7e5-b848-45b1-9379-ea75f531fa2c",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa5ec293-77a1-4c03-85da-752ee4e0e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d175f45d-6ec6-48bd-b700-a780c7afb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30a632e7-f99b-4643-b48b-73dcb98b3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from provided file\n",
    "complaints = pd.read_csv('complaints_25Nov21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76962581-0e0c-4323-946b-da98883e3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "for product in complaints.Product.unique():\n",
    "    y = complaints[['Consumer complaint narrative', 'Product']].dropna()\n",
    "    appended_data.append(y[y.Product == product].sample(2, random_state = 123))\n",
    "\n",
    "limited_df = pd.concat(appended_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "939c2849-cd26-45f4-9a77-9cb76185e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Money transfers', 'Bank account or service', 'Mortgage',\n",
       "       'Consumer Loan', 'Credit card', 'Debt collection', 'Payday loan',\n",
       "       'Prepaid card', 'Credit reporting', 'Student loan',\n",
       "       'Other financial service'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_df.Product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "562390a4-cd82-4550-8864-681fe5d9b81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I got a call from a XXXX XXXX in XXXX, 2014, t...</td>\n",
       "      <td>Money transfers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My husband and I were supposed to close on our...</td>\n",
       "      <td>Money transfers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HiOn XXXX XXXX, 2015 I requested wire transfer...</td>\n",
       "      <td>Bank account or service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My checking and savings account were levied a ...</td>\n",
       "      <td>Bank account or service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am wishing to file a complaint against Wells...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Good afternoon, I filed for chapter XXXX bankr...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I am currently making payments toward a loan I...</td>\n",
       "      <td>Consumer Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Earlier this year, as I was attempting to clea...</td>\n",
       "      <td>Consumer Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>My previous case was case # XXXX. I received a...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I used my Bank of America Travel Rewards card ...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XXXX XXXX are notorious in trying to collect f...</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On XXXX a fraudulent transaction was executed ...</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I APPLIED FOR A LOAN WITH WHOM I THOUGH WAS US...</td>\n",
       "      <td>Payday loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>To Whom It May Concern : I had a short-term ca...</td>\n",
       "      <td>Payday loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I purchased a {$500.00} gift card at XXXX VA. ...</td>\n",
       "      <td>Prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chase Bank has a card called XXXX I applied an...</td>\n",
       "      <td>Prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I 've only received emails from XXXX different...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MY WELLS FARGO CREDIT CARD LIMIT IS INCORRECTL...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The terms of my Chase Private Student Loan wer...</td>\n",
       "      <td>Student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I have XXXX student loans adding up to over {$...</td>\n",
       "      <td>Student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>my account on social security was changed to W...</td>\n",
       "      <td>Other financial service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>In an understanding of an \" integrity and fair...</td>\n",
       "      <td>Other financial service</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Consumer complaint narrative                  Product\n",
       "0   I got a call from a XXXX XXXX in XXXX, 2014, t...          Money transfers\n",
       "1   My husband and I were supposed to close on our...          Money transfers\n",
       "2   HiOn XXXX XXXX, 2015 I requested wire transfer...  Bank account or service\n",
       "3   My checking and savings account were levied a ...  Bank account or service\n",
       "4   I am wishing to file a complaint against Wells...                 Mortgage\n",
       "5   Good afternoon, I filed for chapter XXXX bankr...                 Mortgage\n",
       "6   I am currently making payments toward a loan I...            Consumer Loan\n",
       "7   Earlier this year, as I was attempting to clea...            Consumer Loan\n",
       "8   My previous case was case # XXXX. I received a...              Credit card\n",
       "9   I used my Bank of America Travel Rewards card ...              Credit card\n",
       "10  XXXX XXXX are notorious in trying to collect f...          Debt collection\n",
       "11  On XXXX a fraudulent transaction was executed ...          Debt collection\n",
       "12  I APPLIED FOR A LOAN WITH WHOM I THOUGH WAS US...              Payday loan\n",
       "13  To Whom It May Concern : I had a short-term ca...              Payday loan\n",
       "14  I purchased a {$500.00} gift card at XXXX VA. ...             Prepaid card\n",
       "15  Chase Bank has a card called XXXX I applied an...             Prepaid card\n",
       "16  I 've only received emails from XXXX different...         Credit reporting\n",
       "17  MY WELLS FARGO CREDIT CARD LIMIT IS INCORRECTL...         Credit reporting\n",
       "18  The terms of my Chase Private Student Loan wer...             Student loan\n",
       "19  I have XXXX student loans adding up to over {$...             Student loan\n",
       "20  my account on social security was changed to W...  Other financial service\n",
       "21  In an understanding of an \" integrity and fair...  Other financial service"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb829be4-8fd3-4ed6-a467-00d4c3f5ef9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "032faee7-5fd6-42bf-bbd6-6311e88aeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with customer complaints about financial products. The complaints will be delimited with {delimiter} characters. Classify each complaint into a product category and the customer's emotional state.Provide your output in json format with the keys: product, emotion.\n",
    "\n",
    "Product categories: \n",
    "Money transfers\n",
    "Bank account or service\n",
    "Mortgage\n",
    "Consumer Loan\n",
    "Credit card\n",
    "Debt collection\n",
    "Payday loan\n",
    "Prepaid card\n",
    "Credit reporting\n",
    "Student loan\n",
    "Other financial service\n",
    "\n",
    "Emotion categories:\n",
    "Extremely upset\n",
    "Upset\n",
    "Neutral\n",
    "Not upset\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef041391-667b-4fc0-b856-830a06af6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "complaint = limited_df['Consumer complaint narrative'][n]\n",
    "true_product = limited_df['Product'][n]\n",
    "user_message = f\"\"\"{complaint}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96952ae8-1a1b-49db-8086-85d6e419573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiOn XXXX XXXX, 2015 I requested wire transfer of {$4000.00} to my friend in XXXX XXXX 's XXXX and XXXX XXXX, XXXX Mgr at Wells Fargo Bank XXXX Wisconsin XXXX XXXX XXXX XXXX did wire it on XX/XX/2015 Monday XX/XX/2015. She is telling me to get full details of receiver and atty. XXXX after sending wired sum that I gave in cash. \n",
      "I am unable to get their info and requested that it is Wells fargo Bank 's responsibility as they did contact XXXX XXXX 's office thru their e-mail system and it was quickly replied by his office. \n",
      "I have repeated ly requested XXXX XXXX, Serv Mgr to recall wired sum and credit my cheking account with them. She is giving me run around. \n",
      "Please help get wired sum back in my bank account with them immediately. \n",
      "\n",
      "My name : XXXX XXXX XXXX Address : XXXX XXXX XXXX # XXXX XXXX XXXX, wi-XXXX. \n",
      "Checking A/c # XXXX \n",
      "\n",
      "\n",
      "True Product: Bank account or service \n",
      "\n",
      "\n",
      "{\n",
      "  \"product\": \"Money transfers\",\n",
      "  \"emotion\": \"Extremely upset\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(complaint,'\\n\\n')\n",
    "print('True Product:', true_product, '\\n\\n')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc9f83dc-c0ba-40a0-b5bd-0ada57c4271b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Money transfers</td>\n",
       "      <td>Extremely upset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           product          emotion\n",
       "0  Money transfers  Extremely upset"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "pd.DataFrame(json.loads(completion.choices[0].message.content), index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d96c5-a944-4b17-ad82-77adef2515a0",
   "metadata": {},
   "source": [
    "##### Now it is very easy for me to loop through all my complaints and keep the results in a structured dataframe.  I can also trigger emails based on the identified emotion levels, or do other things as part of my process workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80f644-8ca6-406c-9d68-d71b18fe0d5c",
   "metadata": {},
   "source": [
    "## Understanding the chain of thought \n",
    "\n",
    "You can ask the model to explain how it arrived at its answer by requiring step by step thinking.  You can also specify the steps you want it to take.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2ade533-679f-44e4-8460-9df134579588",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with customer complaints about financial products. The complaints will be delimited with {delimiter} characters. Classify each complaint into a product category and the customer's emotional state. Explain your reasoning step-by-step, marking each step with a step number, and separating step numbers with newline. Summarize the customer complaint in one sentence.  Provide your output in json format with the keys: product, emotion, step-by-step reasoning, summary.\n",
    "\n",
    "Product categories: \n",
    "Money transfers\n",
    "Bank account or service\n",
    "Mortgage\n",
    "Consumer Loan\n",
    "Credit card\n",
    "Debt collection\n",
    "Payday loan\n",
    "Prepaid card\n",
    "Credit reporting\n",
    "Student loan\n",
    "Other financial service\n",
    "\n",
    "Emotion categories:\n",
    "Extremely upset\n",
    "Upset\n",
    "Neutral\n",
    "Not upset\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f413817b-429c-4fe8-9dd4-55986235914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "complaint = limited_df['Consumer complaint narrative'][n]\n",
    "true_product = limited_df['Product'][n]\n",
    "user_message = f\"\"\"{complaint}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c17b6114-506d-45bb-a669-caf6d5e12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiOn XXXX XXXX, 2015 I requested wire transfer of {$4000.00} to my friend in XXXX XXXX 's XXXX and XXXX XXXX, XXXX Mgr at Wells Fargo Bank XXXX Wisconsin XXXX XXXX XXXX XXXX did wire it on XX/XX/2015 Monday XX/XX/2015. She is telling me to get full details of receiver and atty. XXXX after sending wired sum that I gave in cash. \n",
      "I am unable to get their info and requested that it is Wells fargo Bank 's responsibility as they did contact XXXX XXXX 's office thru their e-mail system and it was quickly replied by his office. \n",
      "I have repeated ly requested XXXX XXXX, Serv Mgr to recall wired sum and credit my cheking account with them. She is giving me run around. \n",
      "Please help get wired sum back in my bank account with them immediately. \n",
      "\n",
      "My name : XXXX XXXX XXXX Address : XXXX XXXX XXXX # XXXX XXXX XXXX, wi-XXXX. \n",
      "Checking A/c # XXXX \n",
      "\n",
      "\n",
      "True Product: Bank account or service \n",
      "\n",
      "\n",
      "{\n",
      "  \"product\": \"Money transfers\",\n",
      "  \"emotion\": \"Extremely upset\",\n",
      "  \"step-by-step reasoning\": [\n",
      "    \"Step 1: The customer requested a wire transfer, which falls under the product category 'Money transfers'.\",\n",
      "    \"Step 2: The usage of phrases like 'giving me run around' and 'Please help get wired sum back in my bank account with them immediately' indicate a high level of frustration and urgency, classifying the customer's emotional state as 'Extremely upset'.\",\n",
      "    \"Step 3: The customer is having issues with the transfer they already executed and is not receiving the desired support from the bank, which reinforces the categorization of emotion.\"\n",
      "  ],\n",
      "  \"summary\": \"The customer is extremely upset about a wire transfer issue with Wells Fargo Bank where the bank is not assisting them in recalling a wired sum of $4000 and crediting it back to their checking account.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(complaint,'\\n\\n')\n",
    "print('True Product:', true_product, '\\n\\n')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d127eacb-ccbd-4dac-9dfd-cbc36ec43325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "      <th>summary</th>\n",
       "      <th>step-by-step reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Money transfers</td>\n",
       "      <td>Extremely upset</td>\n",
       "      <td>The customer is extremely upset about a wire t...</td>\n",
       "      <td>[Step 1: The customer requested a wire transfe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           product          emotion  \\\n",
       "0  Money transfers  Extremely upset   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The customer is extremely upset about a wire t...   \n",
       "\n",
       "                              step-by-step reasoning  \n",
       "0  [Step 1: The customer requested a wire transfe...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "pd.DataFrame(json.loads(completion.choices[0].message.content)).groupby(['product', 'emotion', 'summary']).agg({'step-by-step reasoning': lambda x: x.tolist()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3410f9-8653-4f1e-987e-2222779e8f1f",
   "metadata": {},
   "source": [
    "### Validate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0ba2303-d42c-493b-9478-0f3a44d46dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with customer complaints about financial products. The complaints will be delimited with {delimiter} characters. \n",
    "\n",
    "Classify each complaint into a product category and the customer's emotional state. \n",
    "\n",
    "Explain your reasoning step-by-step, marking each step with a step number, and separating step numbers with newline. \n",
    "\n",
    "Summarize the customer complaint in one sentence.  \n",
    "\n",
    "Now check the summary with reference to the original complaint text to confirm if the summary correctly captures the key elements of the complaint.  If the summary is correct, record the result of the check as Y, else N.  \n",
    "\n",
    "Provide your output in json format with the keys: product, emotion, step-by-step reasoning, summary,summary_check.\n",
    "\n",
    "Product categories: \n",
    "Money transfers\n",
    "Bank account or service\n",
    "Mortgage\n",
    "Consumer Loan\n",
    "Credit card\n",
    "Debt collection\n",
    "Payday loan\n",
    "Prepaid card\n",
    "Credit reporting\n",
    "Student loan\n",
    "Other financial service\n",
    "\n",
    "Emotion categories:\n",
    "Extremely upset\n",
    "Upset\n",
    "Neutral\n",
    "Not upset\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d78a0c23-d76d-4f3f-91cd-c7af30b9c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "complaint = limited_df['Consumer complaint narrative'][n]\n",
    "true_product = limited_df['Product'][n]\n",
    "user_message = f\"\"\"{complaint}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53eae063-af66-441e-983a-fc56d69de978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiOn XXXX XXXX, 2015 I requested wire transfer of {$4000.00} to my friend in XXXX XXXX 's XXXX and XXXX XXXX, XXXX Mgr at Wells Fargo Bank XXXX Wisconsin XXXX XXXX XXXX XXXX did wire it on XX/XX/2015 Monday XX/XX/2015. She is telling me to get full details of receiver and atty. XXXX after sending wired sum that I gave in cash. \n",
      "I am unable to get their info and requested that it is Wells fargo Bank 's responsibility as they did contact XXXX XXXX 's office thru their e-mail system and it was quickly replied by his office. \n",
      "I have repeated ly requested XXXX XXXX, Serv Mgr to recall wired sum and credit my cheking account with them. She is giving me run around. \n",
      "Please help get wired sum back in my bank account with them immediately. \n",
      "\n",
      "My name : XXXX XXXX XXXX Address : XXXX XXXX XXXX # XXXX XXXX XXXX, wi-XXXX. \n",
      "Checking A/c # XXXX \n",
      "\n",
      "\n",
      "True Product: Bank account or service \n",
      "\n",
      "\n",
      "{\n",
      "  \"product\": \"Money transfers\",\n",
      "  \"emotion\": \"Upset\",\n",
      "  \"step-by-step reasoning\": [\n",
      "    \"Step 1: Identify key elements of the complaint - The customer mentions a wire transfer of $4000.00 to a friend, which directly relates to the 'Money transfers' category.\",\n",
      "    \"Step 2: Analyze the tone of the complaint - The customer expresses frustration about not being able to get information post-transfer and receiving a runaround when asking for the wired sum to be recalled. This indicates that the customer is upset.\",\n",
      "    \"Step 3: Ensure there is no other financial product involved - The complaint centers on a wire transfer operation and associated service, so it is classified correctly without indication of other financial products.\"\n",
      "  ],\n",
      "  \"summary\": \"The customer is upset because they are experiencing issues with Wells Fargo Bank in getting details after a $4000 wire transfer and the subsequent runaround when requesting a recall of the funds.\",\n",
      "  \"summary_check\": \"Y\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(complaint,'\\n\\n')\n",
    "print('True Product:', true_product, '\\n\\n')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abb6fe-df58-49eb-b978-46e7e6b2f604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ba8d6-eb6b-4a92-a695-617835109ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bc13423-5ef3-48be-a195-f00cae695a3e",
   "metadata": {},
   "source": [
    "#### Analyze customer reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9365e06b-3dcb-4fbd-a195-f14c57fb419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\n",
    "This is amazing. I needed it due to package stealing and living in an apt I wanted safety for my family. \n",
    "There are so many options and upgrades to list. Makes you aware of movement. Has a doorbell to hear, \n",
    "voicemail can be left. App allows for live viewing and speaking. SOS notifications if need. Share and \n",
    "send videos and save them (all w/$3.99+ subscription via Ring) it fit the case I got for the door. \n",
    "The battery holds for a while though it is a pain to recharge. You must remove it from case (I got \n",
    "separate) than unscrew the tiny screw push out battery and charge via the wire charger they provided. \n",
    "It can take 12hrs to charge. Once you receive it open it up right away even if not using to charge it \n",
    "over night. App installation was very easy just follow instructions per app (iPhone/android) it links \n",
    "nicely to the Alexa which will also make you aware someone is at the door. So many variables and options \n",
    "it‚Äôs so good. Even the subscription is cheap. Add many diff ring items and link them all together. \n",
    "Great price newer model 2023 and pretty design. Sleek and modern looking. We love it. Now have piece \n",
    "of mind daily !\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"For the following text, extract the following information: \n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? Y/N\n",
    "sentiment: Positive or negative\n",
    "price_value: Extract any information about value or price. \n",
    "features_liked: Extract key features the customer liked\n",
    "features_not_liked: Extract key features the customer did not like\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "sentiment\n",
    "price_value\n",
    "features_liked\n",
    "features_not_liked\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cddf728-c734-42fb-80f5-23ab28416ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is amazing. I needed it due to package stealing and living in an apt I wanted safety for my family. \n",
      "There are so many options and upgrades to list. Makes you aware of movement. Has a doorbell to hear, \n",
      "voicemail can be left. App allows for live viewing and speaking. SOS notifications if need. Share and \n",
      "send videos and save them (all w/$3.99+ subscription via Ring) it fit the case I got for the door. \n",
      "The battery holds for a while though it is a pain to recharge. You must remove it from case (I got \n",
      "separate) than unscrew the tiny screw push out battery and charge via the wire charger they provided. \n",
      "It can take 12hrs to charge. Once you receive it open it up right away even if not using to charge it \n",
      "over night. App installation was very easy just follow instructions per app (iPhone/android) it links \n",
      "nicely to the Alexa which will also make you aware someone is at the door. So many variables and options \n",
      "it‚Äôs so good. Even the subscription is cheap. Add many diff ring items and link them all together. \n",
      "Great price newer model 2023 and pretty design. Sleek and modern looking. We love it. Now have piece \n",
      "of mind daily !\n",
      " \n",
      "\n",
      "\n",
      "{\n",
      "  \"gift\": \"N\",\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"price_value\": \"Great price, $3.99+ subscription via Ring, cheap subscription\",\n",
      "  \"features_liked\": \"many options and upgrades, awareness of movement, doorbell, voicemail, live viewing, speaking function, SOS notifications, video sharing and saving, fits the case, battery life, easy app installation, compatible with Alexa, sleek and modern design\",\n",
      "  \"features_not_liked\": \"pain to recharge, have to remove battery from case and unscrew, takes 12hrs to charge\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": customer_review}\n",
    "  ]\n",
    ")\n",
    "print(customer_review, '\\n\\n')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789166d-9142-4689-9da4-21f72d62ad2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc8e13-c90a-4811-b5c3-114789b60171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
